{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e94779e",
   "metadata": {},
   "source": [
    "# Goal Switching HW #1: Maximum Likelihood Estimation\n",
    "\n",
    "The goal of maximum likelihood estimation (MLE) is to take a given type of model, and find the parameters that make that model best fit the data. Here, we will use softmax as a practice example. In softmax, the only parameter is Beta (Inverse Tempurature). Here, we will define a `Softmax` class that is a subclass of the `Policy` class. We will define methods for the `Softmax` class such that we can perform maximum likelihood estimation. \n",
    "\n",
    "After this assignment, we will be performing MLE on multiple models to see which one describes the given data set the best. We will also come up with our own models. \n",
    "\n",
    "The exercises will (tentatively) follow the following progression, which will add sequentially more novel material.\n",
    "\n",
    "1. Perform Maximum Likelihood Estimation with a Single Model\n",
    "2. Model Comparison Across Models\n",
    "3. Model Comparison with Simulated Data\n",
    "4. Simulation: Goals without Abandonment\n",
    "5. Simulation: Multi-Faceted Goals\n",
    "6. Simulation: Open-Ended Goals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33d4ec7",
   "metadata": {},
   "source": [
    "First import the necessary libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4181733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from typing import Sequence, Hashable, TypeVar, Tuple, Generic, Container, Set, List, Optional, Union, Dict\n",
    "from collections import defaultdict\n",
    "# from scipy.special import logsumexp\n",
    "# import math\n",
    "from scipy.optimize import minimize\n",
    "# from tqdm import tqdm, trange\n",
    "from dataclasses import dataclass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f5518b",
   "metadata": {},
   "source": [
    "### Inspect the data\n",
    "The first thing to note is that the data is in tidy format, with one row per trial. This makes it easy to work with the data. This is standard in science these days. For more about tidy data, see [this site](https://www.jeannicholashould.com/tidy-data-in-python.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9a76e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>participant</th>\n",
       "      <th>block</th>\n",
       "      <th>net_trial</th>\n",
       "      <th>net_size</th>\n",
       "      <th>net_contents</th>\n",
       "      <th>offer_1</th>\n",
       "      <th>offer_2</th>\n",
       "      <th>offer_3</th>\n",
       "      <th>choice</th>\n",
       "      <th>switch_trial</th>\n",
       "      <th>prop_filled</th>\n",
       "      <th>raw_distance</th>\n",
       "      <th>trials_invested</th>\n",
       "      <th>net_item_idx</th>\n",
       "      <th>goal_offer</th>\n",
       "      <th>decisionRT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.295200</td>\n",
       "      <td>5.270198</td>\n",
       "      <td>4.559294</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>5.295200</td>\n",
       "      <td>5.479601</td>\n",
       "      <td>5.344058</td>\n",
       "      <td>3.756756</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.132380</td>\n",
       "      <td>34.704800</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.479601</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>10.774801</td>\n",
       "      <td>1.702009</td>\n",
       "      <td>4.132068</td>\n",
       "      <td>4.286096</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.269370</td>\n",
       "      <td>29.225199</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.702009</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>4.286096</td>\n",
       "      <td>2.396069</td>\n",
       "      <td>0.924624</td>\n",
       "      <td>4.923793</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.107152</td>\n",
       "      <td>35.713904</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.923793</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>9.209889</td>\n",
       "      <td>2.854140</td>\n",
       "      <td>-0.937678</td>\n",
       "      <td>4.203424</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.230247</td>\n",
       "      <td>30.790111</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.203424</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  participant  block  net_trial  net_size  net_contents  \\\n",
       "0           0            0      1          1        40      0.000000   \n",
       "1           1            0      1          2        40      5.295200   \n",
       "2           2            0      1          3        40     10.774801   \n",
       "3           3            0      1          4        40      4.286096   \n",
       "4           4            0      1          5        40      9.209889   \n",
       "\n",
       "    offer_1   offer_2   offer_3  choice  switch_trial  prop_filled  \\\n",
       "0  5.295200  5.270198  4.559294       1             0     0.000000   \n",
       "1  5.479601  5.344058  3.756756       1             0     0.132380   \n",
       "2  1.702009  4.132068  4.286096       3             1     0.269370   \n",
       "3  2.396069  0.924624  4.923793       3             0     0.107152   \n",
       "4  2.854140 -0.937678  4.203424       3             0     0.230247   \n",
       "\n",
       "   raw_distance  trials_invested  net_item_idx  goal_offer  decisionRT  \n",
       "0     40.000000                0           NaN         NaN         NaN  \n",
       "1     34.704800                1           1.0    5.479601         NaN  \n",
       "2     29.225199                2           1.0    1.702009         NaN  \n",
       "3     35.713904                1           3.0    4.923793         NaN  \n",
       "4     30.790111                2           3.0    4.203424         NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data from the csv file\n",
    "data = pd.read_csv('lab_challenge_full.csv')\n",
    "\n",
    "# dropping some columns i don't think we'll need\n",
    "data = data.drop(columns=['in_scanner', \n",
    "                          'myopic_values_1', \n",
    "                          'myopic_values_2', \n",
    "                          'myopic_values_3',\n",
    "                          'tree_search_item1', \n",
    "                          'tree_search_item2', \n",
    "                          'tree_search_item3',\n",
    "                          'best_alt_idx',\n",
    "                          'best_alt_idx',\n",
    "                          'worst_alt_idx',\n",
    "                          'best_alt_offer',\n",
    "                          'worst_alt_offer'\n",
    "                          ])\n",
    "\n",
    "# take a look at the first few rows of the data\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365f1e10",
   "metadata": {},
   "source": [
    "For now, we will only need to focus on the following columns:\n",
    "- `participant`: the participant number\n",
    "- `block`: the block number which advances each time the participant fills a net with one type of fish\n",
    "- `net_trial`: the number of trials within the given block\n",
    "- `net_size`: the size of the net that the participant tries to fill with **one** type of fish/offer\n",
    "- `net_contents`: how full the net currently is. \n",
    "- `offer_X`: each of the three offers made to the participant corresponding to the three goal options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389cee45",
   "metadata": {},
   "source": [
    "### Define Supporting Classes\n",
    "We will use these to define a probability distribution over actions. These classes are provided for you, and do not need filling out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13aac96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distribution:\n",
    "    def sample(self, rng : random.Random = random):\n",
    "        raise NotImplementedError\n",
    "    def prob(self, x):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class DiscreteDistribution(Distribution):\n",
    "    support : List\n",
    "    def __init__(self, support : List, probs : List):\n",
    "        assert len(support) == len(probs), \"Support and probabilities must be the same length\"\n",
    "        assert abs(sum(probs) - 1) < 1e-6, \"Probabilities must sum to 1\"\n",
    "        items = defaultdict(float)\n",
    "        for x, p in zip(support, probs):\n",
    "            items[x] += p\n",
    "        support, probs = zip(*items.items())\n",
    "        self.support = support\n",
    "        self.probs = probs\n",
    "    def sample(self, rng : random.Random = random):\n",
    "        return rng.choices(self.support, weights=self.probs)[0]\n",
    "    def prob(self, x):\n",
    "        return self.probs[self.support.index(x)]\n",
    "    def items(self):\n",
    "        return zip(self.support, self.probs)\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Discrete(support={self.support}, probs={self.probs})\"\n",
    "    def asdict(self) -> Dict:\n",
    "        return {e: p for e, p in self.items() if p > 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608d54b8",
   "metadata": {},
   "source": [
    "### Define state and action representations\n",
    "Different policies will have different state representations. For a softmax policy, the only information that the state representation will need is each of the three offers on each trial. You will also want to define the action space ahead of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cee64d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Epsilon Greedy, the relevant state on each trial will just be each offer. \n",
    "@dataclass(frozen=True)  # frozen=True makes it immutable\n",
    "class State:\n",
    "    \"\"\"Represents a state with three offer values.\n",
    "    \n",
    "    Attributes:\n",
    "        offer1 (float): First offer value\n",
    "        offer2 (float): Second offer value\n",
    "        offer3 (float): Third offer value\n",
    "    \"\"\"\n",
    "    offer1: float\n",
    "    offer2: float\n",
    "    offer3: float\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validates the state values after initialization.\"\"\"\n",
    "        # Check types\n",
    "        if not all(isinstance(x, (int, float)) for x in (self.offer1, self.offer2, self.offer3)):\n",
    "            raise TypeError(\"All offers must be numbers (int or float)\")\n",
    "    \n",
    "    @property\n",
    "    def as_tuple(self) -> Tuple[float, float, float]:\n",
    "        \"\"\"Returns offers as a tuple.\"\"\"\n",
    "        return (self.offer1, self.offer2, self.offer3)\n",
    "    \n",
    "    @property\n",
    "    def as_array(self) -> np.ndarray:\n",
    "        \"\"\"Returns offers as a numpy array.\"\"\"\n",
    "        return np.array([self.offer1, self.offer2, self.offer3])\n",
    "\n",
    "# We will also define the action space. We do so as an immutable class with the dataclass decorator\n",
    "@dataclass(frozen=True)\n",
    "class ActionSpace:\n",
    "    actions: Tuple[int, int, int]\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if not isinstance(self.actions, tuple):\n",
    "            raise TypeError(\"Actions must be a tuple\")\n",
    "        if len(self.actions) != 3:\n",
    "            raise ValueError(\"Must have exactly 3 actions\")\n",
    "        if not all(isinstance(a, int) for a in self.actions):\n",
    "            raise TypeError(\"All actions must be integers\")\n",
    "        if not all(0 <= a <= 2 for a in self.actions):\n",
    "            raise ValueError(\"All actions must be 0, 1, or 2\")\n",
    "\n",
    "action_space = ActionSpace((0,1,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f3170",
   "metadata": {},
   "source": [
    "### Define a `Policy` class\n",
    "This will serve as the parent class for our `Softmax` class. Notice that we will be able to use this for most any policy that we might come up with. We will be reusing the parent class to define multiple kinds of policies. We will use this class to define the different models we will fit to the data. No need to implement methods here. \n",
    "\n",
    "Notice that many of the methods have already been defined for you, but that some are left undefined. We will fill in the undefined methods in the `Softmax` subclass. The intention here is that as long as the incomplete methods are coded correctly, the rest of the methods in the class will work smoothly for you. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d541820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this abstract for now. Meaning, don't implement the methods. \n",
    "class Policy:\n",
    "    action_space : Sequence[Hashable]\n",
    "    \n",
    "    def __init__(self, name, action_space : ActionSpace):\n",
    "        '''The constructor for the class objects.'''\n",
    "        self.name = name\n",
    "        self.action_space = action_space\n",
    "    \n",
    "    def action_dist(self, s : State, params : List) -> Distribution:\n",
    "        '''\n",
    "        The policy defines a distribution over actions for a given state.\n",
    "        \n",
    "        Args:\n",
    "            s: The current state.\n",
    "            params: A list of relevant parameters. (e.g. epsilon in epsilon greedy)\n",
    "\n",
    "        Returns:\n",
    "            action_dist: a dictionary of keys that are actions and the values are the probability of taking those actions\n",
    "\n",
    "        '''\n",
    "        # Implement this method later for specific policies. \n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def sample_action(self, s : State, params : List, rng : random.Random = random) -> Tuple[int, float]:\n",
    "        '''Sample an action according to the action distribution and return that action and the likelihhod.'''\n",
    "        action_dist = self.action_dist(s, params)\n",
    "        actions, probs = zip(*action_dist.items()) # unpack the dictionary into keys and values\n",
    "        action = rng.choices(actions, weights=probs, k=1)[0] # sample one action accordingly\n",
    "        prob = action_dist[action] # get the probability that action was chosen\n",
    "        return action, prob\n",
    "\n",
    "    def calc_neg_log_lik_single(self, s : State, a : int, params : List) -> float:\n",
    "        '''Takes an action and state and calcs the neg log lik that action happened in that state.'''\n",
    "        action_dist = self.action_dist(s, params).asdict()\n",
    "        neg_log_lik = - np.log(action_dist[a])\n",
    "        return neg_log_lik\n",
    "    \n",
    "    def _create_state_arr(self, data : pd.DataFrame) -> Sequence[State]:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _create_action_arr(self, data : pd.DataFrame) -> Sequence[int]:\n",
    "        '''Create an array of all the action taken.'''\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def calc_neg_log_lik_arr(self, data : pd.DataFrame, params : List) -> Sequence[float]:\n",
    "        '''Calculate the negloglik for each row in a dataset. '''\n",
    "        s_arr = self._create_state_arr(data)\n",
    "        a_arr = self._create_action_arr(data)\n",
    "        assert len(s_arr)==len(a_arr), f'Action array and state array must be same length.'\n",
    "        negloglik_arr = np.zeros(len(s_arr))\n",
    "        negloglik_arr[:] = [self.calc_neg_log_lik_single(s, a, params) \n",
    "                    for s, a in zip(s_arr, a_arr)]\n",
    "        return negloglik_arr\n",
    "    \n",
    "    def calc_neg_log_like_total(self, params : List, data : pd.DataFrame, ):\n",
    "        '''Return the negloglik as a sum total.'''\n",
    "        return np.sum(self.calc_neg_log_lik_arr(data, params))\n",
    "    \n",
    "    def max_lik_estimation(data) -> Tuple[np.ndarray, float]:\n",
    "        '''Uses the likelihood function in the class to estimate the best params given some set of data.\n",
    "        \n",
    "        Args:\n",
    "            data: DataFrame containing trials with columns for offers and choices\n",
    "            \n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "                - np.ndarray: Optimal parameters\n",
    "                - float: Final negative log likelihood\n",
    "        \n",
    "        '''\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2bcc61",
   "metadata": {},
   "source": [
    "### Problem A: Complete the `Softmax` class\n",
    "This policy uses softmax to choose options based off of their offers. Its *parent* class is the `Policy` class, which it *inherets* methods from. Notice that if we define a method again in the *child* class, it overwrites the method (function) from the parent's class. This is called method *overriding*. Here, we are simply overriding methods that were not completed in the `Policy` class. If you do not complete them, but try to run them, it will throw an approprate error that the methods were `NotImplemented`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "51d6a2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Policy):\n",
    "\n",
    "    def __init__(self, action_space: ActionSpace):\n",
    "        super().__init__('Softmax', action_space)\n",
    "        self.n_params = 1 # just inverse tempurature. \n",
    "        \n",
    "    \n",
    "    def action_dist(self, s : State, params : List) -> Distribution:\n",
    "        '''\n",
    "        The policy defines a distribution over actions for a given state.\n",
    "        \n",
    "        Args:\n",
    "            s: The current state.\n",
    "            params: A list of relevant parameters. (e.g. epsilon in epsilon greedy)\n",
    "\n",
    "        Returns:\n",
    "            action_dist: a dictionary of keys that are actions and the values are the probability of taking those actions\n",
    "\n",
    "        '''\n",
    "        ####################################################\n",
    "        # Implement this method. \n",
    "        ####################################################\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def _create_state_arr(self, data : pd.DataFrame) -> Sequence[State]:\n",
    "        '''Create an array of states needed for `action_dist`. One state per trial.'''\n",
    "        ####################################################\n",
    "        # Implement this method. \n",
    "        ####################################################\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _create_action_arr(self, data : pd.DataFrame) -> Sequence[int]:\n",
    "        '''Create an array of all the actions taken. One action per trial'''\n",
    "        ####################################################\n",
    "        # Implement this method. \n",
    "        ####################################################\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def max_lik_estimation(self, data : pd.DataFrame) -> Tuple[float, float]:\n",
    "        ''' Perform maximum likelihood estimation.\n",
    "\n",
    "        Args:\n",
    "            data: DataFrame containing trials with columns for offers and choices\n",
    "\n",
    "        Returns:\n",
    "                Tuple of:\n",
    "                    - Optimal temperature parameter\n",
    "                    - Final negative log likelihood\n",
    "        '''\n",
    "\n",
    "        # we will minimize 5 times, and will take the best of those 5 starts\n",
    "        initial_conds = np.random.uniform(low=1, high=10, size=5) # we must feed the minimizer some initial conditions\n",
    "        bounds = [(0.001, None)] # the bounds for inverse tempurature are (0, inf)\n",
    "        best_result = None\n",
    "        best_nll = np.inf\n",
    "\n",
    "        # minimize for each of the generated intial conditions\n",
    "        for initial_cond in initial_conds:\n",
    "            result = minimize(\n",
    "                self.calc_neg_log_like_total, # find the parameter that minimizes negloglik\n",
    "                args=(data), # this is always passed as a tuple\n",
    "                x0 = [initial_cond], # initial conditions\n",
    "                bounds=bounds, # the space to search with the minimization function\n",
    "                method='L-BFGS-B' # the minimization method. More options out there is this doesn't work well.\n",
    "            )\n",
    "\n",
    "            # take the best minimization so far\n",
    "            if result.success and result.fun < best_nll:\n",
    "                best_result = result\n",
    "                best_nll = result.fun\n",
    "\n",
    "        if best_result is None:\n",
    "                raise RuntimeError(\"Optimization failed from all starting points\")\n",
    "        \n",
    "        return best_result.x[0], best_result.fun # best inv temp, best nll\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f6af72",
   "metadata": {},
   "source": [
    "### Tests\n",
    "Does the code run without errors? There are some assertions that should pass, and will tell you a generic message about what is wrong if they do not pass. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "83be8525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example action distribution:\n",
      "{0: 0.2693074991777379, 1: 0.3289329222889067, 2: 0.4017595785333554}\n",
      "\n",
      "Negloglik Test:\n",
      "1.3119014326242002\n",
      "Negloglik Test:\n",
      "1.1119014326242003\n"
     ]
    }
   ],
   "source": [
    "# create the policy class\n",
    "softmax_policy = Softmax(action_space)\n",
    "\n",
    "# we can test on fake trials\n",
    "state = State(1,2,3)\n",
    "params = [5] # inverse temp\n",
    "action_dist_test = softmax_policy.action_dist(state, params).asdict()\n",
    "print('Example action distribution:')\n",
    "print(action_dist_test)\n",
    "print()\n",
    "\n",
    "# we can also make sure that we are able to calculate neglogliks\n",
    "negloglik_test1 = softmax_policy.calc_neg_log_lik_single(state, 0, params)\n",
    "print('Negloglik Test:')\n",
    "print(negloglik_test1)\n",
    "\n",
    "# we can also make sure that we are able to calculate neglogliks\n",
    "negloglik_test2 = softmax_policy.calc_neg_log_lik_single(state, 1, params)\n",
    "print('Negloglik Test:')\n",
    "print(negloglik_test2)\n",
    "\n",
    "# check that all probabilities for a trial sum to 1\n",
    "assert np.isclose(np.sum(list(action_dist_test.values())), 1), 'For each trial, the sum of the likelihoods of each choice should sum to 1.'\n",
    "\n",
    "# test that likelihood works as expected\n",
    "assert negloglik_test1 > negloglik_test2, 'Actions with lower probability should have higher negloglik'\n",
    "\n",
    "# simple create state/action arr test\n",
    "assert len(softmax_policy._create_state_arr(data))==len(data), 'State array has wrong length.'\n",
    "assert len(softmax_policy._create_action_arr(data))==len(data), 'Action array has wrong length'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82949c04",
   "metadata": {},
   "source": [
    "### Try out calculating negative log likelihood.\n",
    "The total negloglik should be positive number about in the thousands for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1d69d470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7291.406939107112"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_policy.calc_neg_log_like_total(params, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d85219e",
   "metadata": {},
   "source": [
    "### Try running maximum likelihood estimation.\n",
    "This might take a little longer to run, but shouldn't take more than about 1 minute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1b2792df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.065322172870126, 7291.216180944285)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_policy.max_lik_estimation(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cab19ce",
   "metadata": {},
   "source": [
    "## Problem B: Complete Epsilon `Greedy` class\n",
    "This time, create class for an epsilon greedy policy. First, complete the same methods as you did for `Softmax`. Can you use the same methods as you did before? A hint here is that both policies take very similar information (state representations), but have different parameters (`params`). \n",
    "\n",
    "Note that since the polcies are parameterized differently, we will need to re-write the `max_lik_estimation` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749a62f4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b5795848",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Greedy(Policy):\n",
    "\n",
    "    def __init__(self, action_space: ActionSpace):\n",
    "        super().__init__('Softmax', action_space)\n",
    "        self.n_params = 1 # just inverse tempurature. \n",
    "        \n",
    "    \n",
    "    def action_dist(self, s : State, params : List) -> Distribution:\n",
    "        '''\n",
    "        The policy defines a distribution over actions for a given state.\n",
    "        \n",
    "        Args:\n",
    "            s: The current state.\n",
    "            params: A list of relevant parameters. (e.g. epsilon in epsilon greedy)\n",
    "\n",
    "        Returns:\n",
    "            action_dist: a dictionary of keys that are actions and the values are the probability of taking those actions\n",
    "\n",
    "        '''\n",
    "        ####################################################\n",
    "        # Implement this method. The array params will just have a single element: epsilon.\n",
    "        ####################################################\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _create_state_arr(self, data : pd.DataFrame) -> Sequence[State]:\n",
    "        '''Create an array of states needed for `action_dist`. One state per trial.'''\n",
    "        ####################################################\n",
    "        # Implement this method. \n",
    "        ####################################################\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _create_action_arr(self, data : pd.DataFrame) -> Sequence[int]:\n",
    "        '''Create an array of all the actions taken. One action per trial'''\n",
    "        ####################################################\n",
    "        # Implement this method. \n",
    "        ####################################################\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def max_lik_estimation(self, data : pd.DataFrame) -> Tuple[float, float]:\n",
    "        ''' Perform maximum likelihood estimation.\n",
    "\n",
    "        Args:\n",
    "            data: DataFrame containing trials with columns for offers and choices\n",
    "\n",
    "        Returns:\n",
    "                Tuple of:\n",
    "                    - Optimal temperature parameter\n",
    "                    - Final negative log likelihood\n",
    "        '''\n",
    "        ####################################################\n",
    "        # Implement this method. \n",
    "        # Reference the method in the `Softmax` class to scaffold you.\n",
    "        ####################################################\n",
    "        \n",
    "        raise NotImplementedError\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ae9832af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example action distribution:\n",
      "{0: 0.1, 1: 0.1, 2: 0.8}\n",
      "\n",
      "Negloglik Test:\n",
      "2.3025850929940455\n",
      "Negloglik Test:\n",
      "2.3025850929940455\n",
      "Negloglik Test:\n",
      "0.2231435513142097\n"
     ]
    }
   ],
   "source": [
    "# create the policy class\n",
    "greedy_policy = Greedy(action_space)\n",
    "\n",
    "# we can test on fake trials\n",
    "state = State(1,2,3)\n",
    "params = [0.2] # inverse temp\n",
    "action_dist_test = greedy_policy.action_dist(state, params).asdict()\n",
    "print('Example action distribution:')\n",
    "print(action_dist_test)\n",
    "print()\n",
    "\n",
    "# we can also make sure that we are able to calculate neglogliks\n",
    "negloglik_test1 = greedy_policy.calc_neg_log_lik_single(state, 0, params)\n",
    "print('Negloglik Test:')\n",
    "print(negloglik_test1)\n",
    "\n",
    "# we can also make sure that we are able to calculate neglogliks\n",
    "negloglik_test2 = greedy_policy.calc_neg_log_lik_single(state, 1, params)\n",
    "print('Negloglik Test:')\n",
    "print(negloglik_test2)\n",
    "\n",
    "# we can also make sure that we are able to calculate neglogliks\n",
    "negloglik_test3 = greedy_policy.calc_neg_log_lik_single(state, 2, params)\n",
    "print('Negloglik Test:')\n",
    "print(negloglik_test3)\n",
    "\n",
    "# check that all probabilities for a trial sum to 1\n",
    "assert np.isclose(np.sum(list(action_dist_test.values())), 1), 'For each trial, the sum of the likelihoods of each choice should sum to 1.'\n",
    "\n",
    "# check that epsilon greedy assigns same nll to non-greedy options\n",
    "assert negloglik_test1==negloglik_test2, 'Non-greedy options should be chosen with equal likelihood.'\n",
    "\n",
    "# simple create state/action arr test\n",
    "assert len(greedy_policy._create_state_arr(data))==len(data), 'State array has wrong length.'\n",
    "assert len(greedy_policy._create_action_arr(data))==len(data), 'Action array has wrong length'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544ddf84",
   "metadata": {},
   "source": [
    "### Check negative log likelihood (nll) calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "50618e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7762.000227607521"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# does this run okay?\n",
    "greedy_policy.calc_neg_log_like_total(params, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f52c60",
   "metadata": {},
   "source": [
    "### Attempt MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2cb8725f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3564417906661573, 7231.302407928132)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_policy.max_lik_estimation(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6ec951",
   "metadata": {},
   "source": [
    "## Problem C, Part 1\n",
    "In Problem B, we performed maximum likelihood estimation for an epsilon greedy policy. It is worth thinking about what espilon means in this context. Specifically, if people were behaving completely randomly on this task, what value might we expect epsilon to take? (Hint: by traditional notions of epsilong greedy, the answer is not `0.33`). Write your thoughts below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85018ba2",
   "metadata": {},
   "source": [
    "### Response\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c7cf0a",
   "metadata": {},
   "source": [
    "## Problem C, Part 2\n",
    "We do not expect people to be behaving randomly on this task. But what if epsilon were to indicate that people were choosing the highest offer on each trial according to chance? (1) Would this imply that people were behaving randomly? (2) If yes, what are ways we could check to see that were the case? If not, what are alternative explanations that epsilon indicated that people were choosing according to chance, but in fact were not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed3057d",
   "metadata": {},
   "source": [
    "### Response\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23195e7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f678566f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goals-concepts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
